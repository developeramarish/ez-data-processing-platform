# File Processing Refactoring Plan - Corrections & Clarifications

**Document Version:** 1.0  
**Date:** November 10, 2025  
**Supplements:** FILE-PROCESSING-REFACTORING-PLAN.md  
**Status:** Critical Clarifications

---

## üéØ PURPOSE

This document contains important corrections and clarifications to the main implementation plan based on architectural review and existing system analysis.

**Read this BEFORE implementing!**

---

## ‚úÖ CORRECTION 1: Format Preservation Strategy

### Original Plan:
- Reconstruct original format from JSON at output

### CORRECTED APPROACH: HYBRID Strategy

**Implementation:**

```csharp
// FileProcessorService - Store based on file size
if (fileSizeBytes < 10 * 1024 * 1024) // Files < 10MB
{
    // Store BOTH raw and JSON (guaranteed format preservation)
    await hazelcastRawCache.SetAsync($"raw:{guid}", rawContent, TimeSpan.FromHours(1));
    await hazelcastJsonCache.SetAsync($"json:{guid}", jsonContent, TimeSpan.FromHours(1));
}
else // Files >= 10MB
{
    // Store JSON only (reconstruct at output to save memory)
    await hazelcastJsonCache.SetAsync($"json:{guid}", jsonContent, TimeSpan.FromHours(1));
}

await _publishEndpoint.Publish(new ValidationRequestEvent
{
    HazelcastJsonKey = $"json:{guid}",
    HazelcastRawKey = fileSizeBytes < 10 * 1024 * 1024 ? $"raw:{guid}" : null,
    OriginalFormat = format,
    FormatMetadata = metadata,
    FileSizeBytes = fileSizeBytes
});
```

**OutputService - Use raw when available:**

```csharp
if (!string.IsNullOrEmpty(message.HazelcastRawKey))
{
    // Small file - use original raw content (guaranteed identical output)
    var rawContent = await hazelcastRawCache.GetAsync(message.HazelcastRawKey);
    var validRecordIndices = message.ValidRecordIndices; // List of line/record numbers
    var outputContent = FilterValidRecordsFromRaw(rawContent, validRecordIndices, message.OriginalFormat);
}
else
{
    // Large file - reconstruct from JSON
    var validRecordsJson = await hazelcastValidCache.GetAsync(message.HazelcastValidKey);
    var outputContent = await _reconstructor.ReconstructAsync(validRecordsJson, message.FormatMetadata);
}
```

**Benefits:**
- Files <10MB: Byte-for-byte identical output (no reconstruction errors)
- Files ‚â•10MB: Scalable (saves 50% cache memory)
- Flexible: Can adjust threshold based on infrastructure

---

## ‚úÖ CORRECTION 2: Hazelcast Configuration for Development

### Original Plan:
- Mentioned Hazelcast 5.3
- Missing detailed docker-compose configuration

### CORRECTED: Hazelcast 5.5.0 with Full Configuration

**File:** `docker-compose.development.yml`

```yaml
services:
  # ... existing services ...
  
  hazelcast:
    image: hazelcast/hazelcast:5.5.0  # Latest stable
    container_name: hazelcast-dev
    hostname: hazelcast
    ports:
      - "5701:5701"  # Hazelcast port
    environment:
      - JAVA_OPTS=-Xms2g -Xmx2g -Dhazelcast.config=/opt/hazelcast/config/hazelcast.yaml
      - HZ_CLUSTERNAME=data-processing-dev-cluster
      - HZ_NETWORK_JOIN_MULTICAST_ENABLED=false
    volumes:
      - ./deploy/hazelcast/hazelcast-dev.yaml:/opt/hazelcast/config/hazelcast.yaml:ro
    networks:
      - dataprocessing-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5701/hazelcast/health/node-state"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    restart: unless-stopped
```

**Create:** `deploy/hazelcast/hazelcast-dev.yaml`

```yaml
hazelcast:
  cluster-name: data-processing-dev-cluster
  network:
    port:
      auto-increment: true
      port: 5701
    join:
      multicast:
        enabled: false
      tcp-ip:
        enabled: true
        member-list:
          - hazelcast:5701
  map:
    file-content:  # JSON content
      time-to-live-seconds: 3600
      max-idle-seconds: 1800
      eviction:
        eviction-policy: LRU
        max-size-policy: USED_HEAP_PERCENTAGE
        size: 50
    file-content-raw:  # Raw content (for small files)
      time-to-live-seconds: 3600
      max-idle-seconds: 1800
      eviction:
        eviction-policy: LRU
        max-size-policy: USED_HEAP_PERCENTAGE
        size: 50
    valid-records:  # Valid records after validation
      time-to-live-seconds: 3600
      max-idle-seconds: 1800
      eviction:
        eviction-policy: LRU
        max-size-policy: USED_HEAP_PERCENTAGE
        size: 50
```

---

## ‚úÖ CORRECTION 3: Business Metrics Architecture

### Original Plan (WRONG):
- Store data metrics in DataSource.DataMetrics field
- Configure in DataSource form

### CORRECTED: Metrics Already in MetricConfiguration

**Key Insight:** Data/business metrics are ALREADY IMPLEMENTED in `MetricConfiguration` collection!

**Existing Structure:**

```csharp
public class MetricConfiguration : Entity
{
    public string Scope { get; set; } // "global" or "datasource-specific"
    public string? DataSourceId { get; set; } // For datasource-specific metrics
    public string FieldPath { get; set; } // $.amount (JSON path)
    public string PrometheusType { get; set; } // counter, gauge, histogram
    public string? LabelNames { get; set; } // Comma-separated
    public List<AlertRule>? AlertRules { get; set; }
    // ... other fields
}
```

**Frontend Already Has 2 Tabs:**
- Tab 1: Global Metrics (scope="global")
- Tab 2: DataSource-Specific Metrics (scope="datasource-specific", with dataSourceId)

**What ValidationService Needs to Do:**

```csharp
public class ValidationService
{
    private readonly IMetricRepository _metricRepository; // Access to MetricConfiguration
    
    public async Task ValidateAsync(ValidationRequestEvent message)
    {
        // 1. Query BOTH global AND datasource-specific metrics
        var specificMetrics = await _metricRepository.GetByDataSourceIdAsync(message.DataSourceId);
        var globalMetrics = await _metricRepository.GetGlobalMetricsAsync();
        var allMetrics = specificMetrics.Concat(globalMetrics).ToList();
        
        foreach (var record in records)
        {
            if (IsValid(record))
            {
                // 2. Calculate metrics using MetricConfiguration definitions
                foreach (var metric in allMetrics)
                {
                    var value = ExtractFieldValue(record, metric.FieldPath);
                    PublishMetric(metric.Name, value, metric.LabelNames, dataSource);
                }
            }
        }
    }
}
```

**Changes to Plan:**

**REMOVE:**
- ‚ùå Phase 2.5: Adding DataMetrics to DataSource entity (NOT NEEDED)
- ‚ùå Phase 10.1: Data Metrics UI in DataSource form (ALREADY EXISTS in metrics wizard)
- ‚ùå Phase 10.2: DataMetricEditor component (ALREADY EXISTS as MetricConfigurationWizard)

**KEEP/ADD:**
- ‚úÖ Phase 5: ValidationService queries MetricConfiguration and calculates
- ‚úÖ Add IMetricRepository injection to ValidationService
- ‚úÖ Query for global + specific metrics during validation

---

## ‚úÖ CORRECTION 4: Output Configuration

### Original Plan:
- Simple strings: OutputKafkaTopic, OutputLocalPath

### CORRECTED: Nested OutputConfiguration Object

**File:** `src/Services/Shared/Entities/DataProcessingDataSource.cs`

```csharp
public class DataProcessingDataSource : Entity
{
    // ... existing fields ...
    
    // NEW: Output configuration (nested object)
    public OutputConfiguration? Output { get; set; }
}

public class OutputConfiguration
{
    public string? KafkaTopic { get; set; }
    public string? LocalFolderPath { get; set; }
    public string OutputFormat { get; set; } = "original"; // "original", "json", "csv"
    public bool IncludeInvalidRecords { get; set; } = false;
    public Dictionary<string, string>? CustomHeaders { get; set; } // For Kafka
}
```

**Frontend: Add Output Tab**

**Create:** `src/Frontend/src/components/datasource/tabs/OutputTab.tsx`

```typescript
export const OutputTab: React.FC<OutputTabProps> = ({ output, onChange }) => {
  return (
    <Card title="◊ô◊¢◊ì◊ô ◊§◊ú◊ò">
      <Form layout="vertical">
        <Form.Item label="Kafka Topic">
          <Input
            value={output?.kafkaTopic}
            placeholder="e.g., validated-records"
          />
        </Form.Item>
        
        <Form.Item label="◊™◊ô◊ß◊ô◊ô◊î ◊û◊ß◊ï◊û◊ô◊™">
          <Input
            value={output?.localFolderPath}
            placeholder="e.g., /output/validated"
          />
        </Form.Item>
        
        <Form.Item label="◊§◊ï◊®◊û◊ò ◊§◊ú◊ò">
          <Select
            value={output?.outputFormat || 'original'}
            options={[
              { label: '◊§◊ï◊®◊û◊ò ◊û◊ß◊ï◊®◊ô', value: 'original' },
              { label: 'JSON', value: 'json' },
              { label: 'CSV', value: 'csv' }
            ]}
          />
        </Form.Item>
        
        <Form.Item>
          <Space>
            <Switch checked={output?.includeInvalidRecords} />
            <Text>◊õ◊ï◊ú◊ú ◊®◊©◊ï◊û◊ï◊™ ◊©◊í◊ï◊ô◊ï◊™</Text>
          </Space>
        </Form.Item>
      </Form>
    </Card>
  );
};
```

**Update:** `src/Frontend/src/pages/datasources/DataSourceFormEnhanced.tsx`

Add new tab after Notifications:
```typescript
{
  key: 'output',
  label: '◊§◊ú◊ò',
  children: <OutputTab output={formData.output} onChange={handleOutputChange} />
}
```

**DemoDataGenerator:** Add output config to generated datasources

```csharp
Output = new OutputConfiguration
{
    KafkaTopic = $"validated-{datasourceName.ToLower()}",
    LocalFolderPath = $"C:\\DataProcessing\\output\\{datasourceName}",
    OutputFormat = "original",
    IncludeInvalidRecords = false
}
```

---

## üìã REVISED PHASE SUMMARY

### Phases That Changed:

| Phase | Original | Corrected |
|-------|----------|-----------|
| **Phase 1** | Hazelcast 5.3 basic | ‚úÖ Hazelcast 5.5.0 with full config + hazelcast-dev.yaml |
| **Phase 2.5** | Add DataMetrics to DataSource | ‚úÖ Add OutputConfiguration only (metrics already in MetricConfiguration) |
| **Phase 4** | Store JSON only | ‚úÖ HYBRID: Store raw+JSON for <10MB files |
| **Phase 5** | Calculate metrics from DataSource.DataMetrics | ‚úÖ Query MetricConfiguration (global + specific) |
| **Phase 10** | Add data metrics UI to DataSource form | ‚úÖ Add Output tab only (metrics UI already exists) |

---

## üìä UPDATED TASK BREAKDOWN

### Tasks to REMOVE from Task Manager:
- ‚ùå task-26: "Frontend Data Metrics Configuration" (already exists)
- ‚ùå Part of task-16: "Add DataMetrics to entity" (not needed)

### Tasks to ADD:
- ‚úÖ Create `deploy/hazelcast/hazelcast-dev.yaml`
- ‚úÖ Create `src/Frontend/src/components/datasource/tabs/OutputTab.tsx`
- ‚úÖ Update ValidationService to inject IMetricRepository
- ‚úÖ Implement hybrid caching strategy in FileProcessorService

### Tasks to UPDATE:
- task-12: Add message fields for raw content key
- task-16: Add OutputConfiguration (remove DataMetrics)
- task-19: Query MetricConfiguration instead of DataSource for metrics
- task-27: Change to "Create Output Tab" (not data metrics tab)

---

## üîç KEY ARCHITECTURAL INSIGHTS

### 1. **Metrics System is Complete**
- Global metrics (Tab 1): Apply to ALL datasources
- Specific metrics (Tab 2): Apply to ONE datasource
- Both stored in `MetricConfiguration` collection
- Both calculated by ValidationService during validation
- Frontend UI already complete in MetricConfigurationWizard

### 2. **ValidationService Metric Calculation**

```csharp
// Pseudocode for ValidationService enhancement
public async Task ValidateFileAsync(ValidationRequestEvent message)
{
    // Get datasource
    var dataSource = await GetDataSourceAsync(message.DataSourceId);
    
    // Get BOTH types of metrics
    var specificMetrics = await _metricRepo.GetByDataSourceIdAsync(message.DataSourceId);
    // Returns: WHERE Scope = "datasource-specific" AND DataSourceId = message.DataSourceId
    
    var globalMetrics = await _metricRepo.GetGlobalMetricsAsync();
    // Returns: WHERE Scope = "global"
    
    var allMetrics = specificMetrics.Concat(globalMetrics).ToList();
    
    // Validate records
    foreach (var record in records)
    {
        if (await ValidateAgainstSchemaAsync(record))
        {
            validRecords.Add(record);
            
            // Calculate ALL metrics (global + specific) for this record
            await CalculateMetricsAsync(record, allMetrics, dataSource);
        }
    }
}

private async Task CalculateMetricsAsync(
    JsonDocument record,
    List<MetricConfiguration> metrics,
    DataProcessingDataSource dataSource)
{
    foreach (var metric in metrics)
    {
        // Use metric.FieldPath from MetricConfiguration
        var fieldValue = ExtractFieldValue(record, metric.FieldPath);
        
        // Build labels from metric.LabelNames
        var labels = new List<KeyValuePair<string, object>>
        {
            new("datasource_id", dataSource.ID),
            new("datasource_name", dataSource.Name),
            new("metric_name", metric.DisplayName)
        };
        
        if (!string.IsNullOrEmpty(metric.LabelNames))
        {
            foreach (var labelName in metric.LabelNames.Split(','))
            {
                labels.Add(new(labelName.Trim(), ExtractLabelValue(record, labelName.Trim())));
            }
        }
        
        // Publish to OpenTelemetry based on metric.PrometheusType
        PublishToOpenTelemetry(metric.Name, fieldValue, labels, metric.PrometheusType);
    }
}
```

### 3. **Output Configuration**
- Stored as nested object in DataSource (OutputConfiguration)
- Managed in NEW "Output" tab in DataSource form
- Includes: Kafka topic, local folder, format preference, include invalid flag
- DemoDataGenerator creates test output configs

---

## üìã IMPLEMENTATION CHECKLIST

Before starting implementation, verify understanding:

- [ ] Hazelcast 5.5.0 with config file
- [ ] Hybrid caching (raw+JSON for small, JSON for large)
- [ ] Metrics in MetricConfiguration (NOT in DataSource)
- [ ] ValidationService queries MetricConfiguration
- [ ] Global metrics apply to all datasources
- [ ] Specific metrics apply to one datasource
- [ ] Output tab in DataSource form (NOT metrics tab)
- [ ] DemoDataGenerator creates output configs

---

## üìû QUESTIONS FOR CLARIFICATION

Before proceeding with implementation:

1. **File size threshold:** Confirm 10MB threshold for hybrid caching OK?
2. **Hazelcast maps:** 3 maps (file-content, file-content-raw, valid-records) sufficient?
3. **Output format preference:** Allow user to force JSON output even if input was CSV?
4. **Invalid records inclusion:** For debugging, include invalid records in output file?

---

**This document must be read in conjunction with FILE-PROCESSING-REFACTORING-PLAN.md**

---

**END OF CORRECTIONS**
