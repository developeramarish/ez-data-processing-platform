# FileProcessor Horizontal Scaling Test Report

**Date:** December 29, 2025 (Session 35)
**Status:** PASSED

---

## Test Objective

Verify that FileProcessor can run with multiple replicas and correctly distribute load across instances using RabbitMQ competing consumers pattern.

---

## Configuration

### FileProcessor Deployment

```yaml
replicas: 2
image: fileprocessor:v2
```

### MassTransit Consumer Configuration

```csharp
x.AddConsumer<FileDiscoveredEventConsumer>(cfg =>
{
    cfg.UseConcurrentMessageLimit(10); // 10 files concurrent per instance
});

x.UsingRabbitMq((context, cfg) =>
{
    cfg.Host(rabbitMqHost, "/", h => { ... });
    cfg.ConfigureEndpoints(context); // Creates competing consumers
});
```

### Infrastructure

| Component | Configuration |
|-----------|--------------|
| RabbitMQ | Single instance with competing consumer pattern |
| Hazelcast | Distributed cache (`file-content`, `valid-records` maps) |
| Pods | 2 replicas running simultaneously |

---

## Test Results

### Pod Status

```
NAME                             READY   STATUS    RESTARTS   AGE
fileprocessor-5f695d8999-z7cbz   1/1     Running   0          38m
fileprocessor-5f695d8999-zl747   1/1     Running   0          38m
```

### Load Distribution (2-minute window)

| Pod | Files Processed | Percentage |
|-----|-----------------|------------|
| Pod 1 (z7cbz) | 1,306 | 50% |
| Pod 2 (zl747) | 1,306 | 50% |
| **Total** | **2,612** | **100%** |

### Hazelcast Shared Cache

| Map | Entries |
|-----|---------|
| file-content | 6,654 |
| valid-records | 0 (consumed) |

Both pods successfully wrote to and read from the shared Hazelcast cluster.

### Output Verification

- ValidSchemaTest: 100/100 output files generated
- No data loss or duplication observed
- All files processed end-to-end

---

## Architecture Validation

### What Works

1. **RabbitMQ Competing Consumers** - MassTransit automatically creates a single queue that both consumers compete for. Messages are distributed round-robin.

2. **Hazelcast Distributed Cache** - Both pods connect to the same Hazelcast cluster and share the `file-content` map without conflicts.

3. **Concurrent Processing** - Each pod processes up to 10 files concurrently (`UseConcurrentMessageLimit(10)`), giving 20 total concurrent files with 2 replicas.

4. **Deduplication** - File hash maps (`file-hashes-{datasourceId}`) work correctly across pods - no duplicate processing observed.

### Message Flow

```
FileDiscovery → RabbitMQ Queue → FileProcessor Pods (competing)
                                      ↓
                               Hazelcast Cache (shared)
                                      ↓
                               Validation → Output
```

---

## Performance Implications

### Capacity with 2 Replicas

| Metric | Single Replica | 2 Replicas |
|--------|----------------|------------|
| Concurrent files | 10 | 20 |
| Throughput (est.) | ~100 files/min | ~200 files/min |
| Memory per instance | ~512MB | ~512MB x 2 |

### Scaling Recommendations

| Load Level | Recommended Replicas |
|------------|---------------------|
| < 500 files/hour | 1 |
| 500-2000 files/hour | 2 |
| 2000-5000 files/hour | 3 |
| > 5000 files/hour | 4+ with monitoring |

---

## Configuration Changes Applied

The FileProcessor deployment already had 2 replicas configured. No changes were needed.

### Deployment Manifest

File: `k8s/deployments/fileprocessor.yaml`

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: fileprocessor
  namespace: ez-platform
spec:
  replicas: 2  # Horizontal scaling enabled
  selector:
    matchLabels:
      app: fileprocessor
  template:
    spec:
      containers:
      - name: fileprocessor
        image: fileprocessor:v2
        resources:
          requests:
            cpu: 50m
            memory: 128Mi
          limits:
            cpu: 500m
            memory: 512Mi
```

---

## Conclusion

**PASSED** - FileProcessor horizontal scaling works correctly:

1. Multiple replicas receive balanced load distribution (50/50)
2. RabbitMQ competing consumers pattern works as expected
3. Hazelcast distributed cache is shared correctly
4. No data loss or duplication
5. System ready for production scaling

---

## Related Documentation

- [STRESS-TEST-1000-FILES-REPORT.md](STRESS-TEST-1000-FILES-REPORT.md) - Stress test results
- [VALID-SCHEMA-TEST-REPORT.md](VALID-SCHEMA-TEST-REPORT.md) - Validation test
- [k8s/deployments/fileprocessor.yaml](../../k8s/deployments/fileprocessor.yaml) - Deployment config

---

*Report generated: December 29, 2025*
